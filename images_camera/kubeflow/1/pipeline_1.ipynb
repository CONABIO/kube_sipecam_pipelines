{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1ef5bd32584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "from conabio_ml.datasets.dataset import Partitions\n",
    "from conabio_ml.utils.dataset_utils import read_labelmap_file, write_labelmap_file\n",
    "\n",
    "from conabio_ml_vision.evaluator.evaluator import ImageClassificationEvaluator\n",
    "from conabio_ml_vision.evaluator.evaluator import ImageClassificationMetrics\n",
    "from conabio_ml_vision.datasets.datasets import ImageDataset, ImagePredictionDataset\n",
    "\n",
    "from conabio_ml.utils.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def classify_dataset(dataset,\n",
    "                     classifs_csv,\n",
    "                     labelmap,\n",
    "                     model,\n",
    "                     images_size,\n",
    "                     batch_size,\n",
    "                     partition=Partitions.TEST,\n",
    "                     max_classifs=1):\n",
    "    if os.path.isfile(classifs_csv):\n",
    "        return ImagePredictionDataset.from_csv(source_path=classifs_csv)\n",
    "\n",
    "    test_df = dataset.get_splitted(partitions=partition)\n",
    "    test_batches = ImageDataGenerator().flow_from_dataframe(\n",
    "        test_df,\n",
    "        x_col=\"item\",\n",
    "        class_mode=None,\n",
    "        target_size=images_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        validate_filenames=False)\n",
    "\n",
    "    preds = model.predict(test_batches, batch_size=batch_size)\n",
    "    results = defaultdict(list)\n",
    "    for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "        sorted_inds = [y[0] for y in sorted(enumerate(preds[i]), key=lambda x:x[1], reverse=True)]\n",
    "        for k in range(max_classifs):\n",
    "            ind = sorted_inds[k]\n",
    "            results[\"item\"].append(row[\"item\"])\n",
    "            results[\"label\"].append(labelmap[ind])\n",
    "            results[\"score\"].append(preds[i][ind])\n",
    "            results[\"image_id\"].append(row[\"image_id\"])\n",
    "            results[\"id\"].append(row[\"id\"])\n",
    "    data = pd.DataFrame(results)\n",
    "    images_dir = dataset.get_images_dir()\n",
    "    classifs_on_test_ds = ImagePredictionDataset(data, info={}, images_dir=images_dir)\n",
    "    classifs_on_test_ds.to_csv(dest_path=classifs_csv)\n",
    "    return classifs_on_test_ds\n",
    "\n",
    "\n",
    "def eval_multi(true_ds, pred_ds, eval_dir, partition=Partitions.TEST):\n",
    "    res_eval = ImageClassificationEvaluator.eval(\n",
    "        dataset_true=true_ds,\n",
    "        dataset_pred=pred_ds,\n",
    "        eval_config={\n",
    "            'metrics_set': {\n",
    "                ImageClassificationMetrics.Sets.MULTICLASS: {\n",
    "                    \"average\": 'macro',\n",
    "                    \"normalize\": \"true\",\n",
    "                    \"zero_division\": 1\n",
    "                }\n",
    "            },\n",
    "            \"labels\": true_ds.get_categories(),\n",
    "            'partition': partition,\n",
    "        })\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    res_eval.result_plots(dest_path=eval_dir, report=True, sample_counts_in_bars=True)\n",
    "    res_eval.store_eval_metrics(dest_path=os.path.join(eval_dir, \"results.json\"))\n",
    "\n",
    "def train(model_checkpoint_path,\n",
    "          dataset,\n",
    "          labelmap_path,\n",
    "          model_type,\n",
    "          model_name,\n",
    "          images_size,\n",
    "          epochs,\n",
    "          batch_size):\n",
    "    if not os.path.isfile(model_checkpoint_path):\n",
    "        df_train = dataset.get_splitted(partitions=Partitions.TRAIN)\n",
    "        n_cats = dataset.get_num_categories()\n",
    "        classes = dataset.get_classes()\n",
    "        train_batches = ImageDataGenerator().flow_from_dataframe(df_train,\n",
    "                                                                 x_col=\"item\",\n",
    "                                                                 y_col=\"label\",\n",
    "                                                                 classes=classes,\n",
    "                                                                 target_size=images_size,\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 validate_filenames=False)\n",
    "        labelmap = {v: k for k, v in train_batches.class_indices.items()}\n",
    "        write_labelmap_file(labelmap=labelmap, dest_path=labelmap_path)\n",
    "        with tf.distribute.MirroredStrategy().scope():\n",
    "            model = build_model(num_classes=n_cats, model_type=model_type,\n",
    "                                name=model_name, input_shape=(images_size[0], images_size[1], 3))\n",
    "        logger.info(f\"Training model {model_name}\")\n",
    "        model.fit(train_batches, epochs=epochs, verbose=1,\n",
    "                  callbacks=[ModelCheckpoint(model_checkpoint_path)])\n",
    "    else:\n",
    "        pass # model = load_model_from_ckp(model_checkpoint_path)\n",
    "\n",
    "    # return model\n",
    "\n",
    "def load_model_from_ckp(model_checkpoint_path):\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        return load_model(model_checkpoint_path)\n",
    "\n",
    "def build_model(num_classes, model_type, name, input_shape):\n",
    "    img_augmentation = Sequential(\n",
    "        [\n",
    "            preprocessing.RandomRotation(factor=0.15),\n",
    "            preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "            preprocessing.RandomFlip(mode=\"horizontal\"),\n",
    "            preprocessing.RandomContrast(factor=0.1),\n",
    "        ],\n",
    "        name=\"img_augmentation\",\n",
    "    )\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = img_augmentation(inputs)\n",
    "    model = model_type(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "    model.trainable = False\n",
    "    x = GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2, name=\"top_dropout\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "    model = Model(inputs, outputs, name=name)\n",
    "    optimizer = Adam(learning_rate=1e-2)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "RANDOM_STATE = 445\n",
    "BATCH_SIZE_EVAL = 16\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "TRAIN_PERC = 0.8\n",
    "\n",
    "BASE_PATH = '/shared_volume/ecoinf_tests/kale_aws/'\n",
    "\n",
    "# Results\n",
    "results_path = os.path.join(BASE_PATH, \"results\", \"pipeline_1\")\n",
    "dataset_csv = os.path.join(results_path, \"dataset.csv\")\n",
    "classifs_csv = os.path.join(results_path, \"classifs_on_test_part.csv\")\n",
    "model_checkpoint_path = os.path.join(results_path, \"efficientnet_b3_train_1.model.hdf5\")\n",
    "labelmap_file = os.path.join(results_path, \"labels.txt\")\n",
    "eval_dir = os.path.join(results_path, \"evaluation\")\n",
    "# Data\n",
    "snmb_images_dir = os.path.join(BASE_PATH, 'data', \"snmb\")\n",
    "snmb_crops_dir = os.path.join(BASE_PATH, 'data', \"snmb_crops\")\n",
    "# Files\n",
    "snmb_json = os.path.join(BASE_PATH, \"files\", \"snmb_2021_detection-bboxes.json\")\n",
    "mappings_csv = os.path.join(BASE_PATH, \"files\", \"snmb_to_wcs_compet.csv\")\n",
    "compet_labelmap_file = os.path.join(BASE_PATH, \"files\", \"compet_labels.txt\")\n",
    "\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "block:dataset_creation"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-112e0589e7ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msnmb_crops_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcompet_labelmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_labelmap_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompet_labelmap_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset creation\n",
    "if os.path.isfile(dataset_csv):\n",
    "    dataset = ImageDataset.from_csv(source_path=dataset_csv, images_dir=snmb_crops_dir)\n",
    "else:\n",
    "    compet_labelmap = read_labelmap_file(compet_labelmap_file)\n",
    "    dataset = ImageDataset.from_json(\n",
    "        snmb_json,\n",
    "        images_dir=snmb_images_dir,\n",
    "        categories=list(compet_labelmap.values()),\n",
    "        exclude_categories=['empty'],\n",
    "        mapping_classes=mappings_csv,\n",
    "        not_exist_ok=True)\n",
    "    dataset = dataset.create_classif_ds_from_bboxes_crops(\n",
    "        dest_path=snmb_crops_dir, include_id=True, inherit_fields=[\"image_id\", 'location'])\n",
    "    dataset.split(train_perc=TRAIN_PERC, test_perc=1.-TRAIN_PERC,\n",
    "                  val_perc=0, group_by_field=\"location\")\n",
    "    dataset.to_csv(dest_path=dataset_csv, columns=[\"image_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:fit_model",
     "prev:dataset_creation",
     "limit:nvidia.com/gpu:1"
    ]
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "train(model_checkpoint_path,\n",
    "      dataset,\n",
    "      labelmap_file,\n",
    "      model_type=EfficientNetB3,\n",
    "      model_name='EfficientNetB3',\n",
    "      images_size=(300, 300),\n",
    "      epochs=EPOCHS,\n",
    "      batch_size=BATCH_SIZE_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:classification",
     "prev:fit_model",
     "limit:nvidia.com/gpu:1"
    ]
   },
   "outputs": [],
   "source": [
    "# Inference on Test partition\n",
    "model = load_model_from_ckp(model_checkpoint_path)\n",
    "classifs_ds = classify_dataset(dataset,\n",
    "                               classifs_csv,\n",
    "                               read_labelmap_file(labelmap_file),\n",
    "                               model,\n",
    "                               images_size=(300, 300),\n",
    "                               batch_size=BATCH_SIZE_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:evaluation",
     "prev:classification"
    ]
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "eval_multi(dataset, classifs_ds, eval_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "sipecam/ecoinf-kale-gpu:0.6.1",
   "experiment": {
    "id": "new",
    "name": "ecoinf-exps-1"
   },
   "experiment_name": "ecoinf-exps-1",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "Prueba de entrenamiento-clasificacion-evaluacion. Aqui se usan las 10 K fotos",
   "pipeline_name": "test-3",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/shared_volume",
     "name": "efs",
     "size": 1,
     "size_type": "Gi",
     "snapshot": false,
     "snapshot_name": "",
     "type": "pvc"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
